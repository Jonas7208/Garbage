import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
import json
import sys


class Config:
    """Zentrale Konfiguration"""
    IMG_SIZE = (299, 299)
    BATCH_SIZE = 32
    ADDITIONAL_EPOCHS = 10
    LEARNING_RATE = 0.00005
    TRAIN_DIR = 'dataset/train'
    NUM_CLASSES = 6
    MODEL_PATH = 'models/best_model.keras'

    # Neue Modellpfade
    RESUMED_BEST = 'models/resumed_best_model.keras'
    FINAL_RESUMED = 'models/final_resumed_model.keras'

    # Fine-Tuning Einstellungen
    UNFREEZE_FROM_LAYER = 200
    AUTO_UNFREEZE = False  # Automatisch entfrieren ohne Nachfrage


def setup_gpu():
    """Konfiguriere GPU fÃ¼r optimale Performance"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"ðŸŽ® GPU gefunden: {len(gpus)} GerÃ¤t(e)")
            return True
        except RuntimeError as e:
            print(f"âš ï¸ GPU-Konfiguration fehlgeschlagen: {e}")
    else:
        print("âš ï¸ Keine GPU gefunden - Training lÃ¤uft auf CPU")
    return False


def validate_directories(config):
    """ÃœberprÃ¼fe ob alle benÃ¶tigten Verzeichnisse existieren"""
    train_path = Path(config.TRAIN_DIR)
    model_path = Path(config.MODEL_PATH)

    if not train_path.exists():
        raise FileNotFoundError(f"âŒ Trainingsverzeichnis nicht gefunden: {train_path}")

    if not model_path.exists():
        raise FileNotFoundError(f"âŒ Modell nicht gefunden: {model_path}")

    # Erstelle Ausgabeverzeichnisse falls nicht vorhanden
    Path('models').mkdir(exist_ok=True)
    Path('logs/fit').mkdir(parents=True, exist_ok=True)

    print("âœ… Alle Verzeichnisse validiert")


def load_model(model_path):
    """Lade Modell mit Fehlerbehandlung"""
    try:
        model = tf.keras.models.load_model(model_path)
        print(f"âœ… Modell geladen von: {model_path}")

        # Zeige Modell-Informationen
        print("\nðŸ“Š Modell-Info:")
        print(f"   Total Parameters: {model.count_params():,}")

        # Korrekte Methode fÃ¼r trainierbare Parameter
        trainable_count = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        print(f"   Trainable Parameters: {trainable_count:,}")

        return model
    except Exception as e:
        raise RuntimeError(f"âŒ Fehler beim Laden des Modells: {e}")


def create_data_generators(config):
    """Erstelle Daten-Generatoren mit Fehlerbehandlung"""
    print("\nðŸ“Š Bereite Daten vor...")

    train_datagen = ImageDataGenerator(
        rescale=1. / 255,
        validation_split=0.2,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    val_datagen = ImageDataGenerator(
        rescale=1. / 255,
        validation_split=0.2
    )

    try:
        train_generator = train_datagen.flow_from_directory(
            config.TRAIN_DIR,
            target_size=config.IMG_SIZE,
            batch_size=config.BATCH_SIZE,
            class_mode='categorical',
            subset='training',
            shuffle=True
        )

        validation_generator = val_datagen.flow_from_directory(
            config.TRAIN_DIR,
            target_size=config.IMG_SIZE,
            batch_size=config.BATCH_SIZE,
            class_mode='categorical',
            subset='validation',
            shuffle=False
        )

        print(f"âœ… Trainingssamples: {train_generator.samples}")
        print(f"âœ… Validierungssamples: {validation_generator.samples}")
        print(f"âœ… Klassen: {list(train_generator.class_indices.keys())}")

        return train_generator, validation_generator

    except Exception as e:
        raise RuntimeError(f"âŒ Fehler beim Erstellen der Generatoren: {e}")


def calculate_class_weights(train_generator, config):
    """Berechne Class Weights fÃ¼r unbalancierte Daten"""
    print("\nâš–ï¸ Berechne Class Weights...")

    class_counts = np.zeros(config.NUM_CLASSES)
    for class_name, class_idx in train_generator.class_indices.items():
        class_dir = Path(config.TRAIN_DIR) / class_name
        class_counts[class_idx] = len(list(class_dir.iterdir()))

    total_samples = class_counts.sum()
    class_weights = {
        i: total_samples / (config.NUM_CLASSES * count)
        for i, count in enumerate(class_counts)
    }

    # Zeige Class Distribution
    print("\nðŸ“Š Klassenverteilung:")
    for class_name, class_idx in train_generator.class_indices.items():
        print(f"   {class_name}: {int(class_counts[class_idx])} Bilder (Weight: {class_weights[class_idx]:.2f})")

    return class_weights


def unfreeze_layers(model, config):
    """Entfiere mehr Layers fÃ¼r besseres Fine-Tuning"""
    print("\nðŸ”“ Fine-Tuning Konfiguration...")

    # Automatische oder manuelle Entscheidung
    if not config.AUTO_UNFREEZE:
        user_input = input("MÃ¶chtest du mehr Layers trainieren? (j/n) [Standard: n]: ").lower()
        if user_input not in ['j', 'ja', 'y', 'yes']:
            print("â­ï¸ Ãœberspringe Layer-Entfrierung")
            return

    # Finde Base Model (InceptionV3 oder Ã¤hnlich)
    base_model_found = False
    for layer in model.layers:
        if hasattr(layer, 'layers') and len(layer.layers) > config.UNFREEZE_FROM_LAYER:
            base_model_found = True
            print(f"âœ… Base Model gefunden: {layer.name} mit {len(layer.layers)} Layers")

            # Entfiere Layers ab bestimmtem Index
            for sublayer in layer.layers[config.UNFREEZE_FROM_LAYER:]:
                sublayer.trainable = True

            trainable_layers = len([l for l in layer.layers if l.trainable])
            print(f"âœ… {trainable_layers} Layers des Base Models sind jetzt trainierbar")
            break

    if not base_model_found:
        print("âš ï¸ Kein Base Model mit genug Layers gefunden")

    # ZÃ¤hle trainierbare Parameter
    trainable_count = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
    print(f"   Neue trainierbare Parameter: {trainable_count:,}")


def setup_callbacks(config):
    """Konfiguriere Training Callbacks"""
    print("\nâš™ï¸ Konfiguriere Callbacks...")

    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    log_dir = f"logs/fit/{timestamp}_resume"

    callbacks = [
        ModelCheckpoint(
            config.RESUMED_BEST,
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        ),
        TensorBoard(
            log_dir=log_dir,
            histogram_freq=1
        )
    ]

    return callbacks, log_dir


def plot_training_history(history, save_path='training_history_resumed.png'):
    """Visualisiere Trainingsverlauf"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Training')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # Loss
    axes[0, 1].plot(history.history['loss'], label='Training')
    axes[0, 1].plot(history.history['val_loss'], label='Validation')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Top-2 Accuracy
    if 'top_2_accuracy' in history.history:
        axes[1, 0].plot(history.history['top_2_accuracy'], label='Training')
        axes[1, 0].plot(history.history['val_top_2_accuracy'], label='Validation')
        axes[1, 0].set_title('Top-2 Accuracy')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Top-2 Accuracy')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

    # Learning Rate
    if 'lr' in history.history:
        axes[1, 1].plot(history.history['lr'])
        axes[1, 1].set_title('Learning Rate')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('LR')
        axes[1, 1].set_yscale('log')
        axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nðŸ“Š Trainingshistorie gespeichert: {save_path}")
    plt.close()


def save_training_summary(results_before, results_after, config, log_dir):
    """Speichere Trainings-Zusammenfassung als JSON"""
    summary = {
        'timestamp': datetime.now().isoformat(),
        'config': {
            'additional_epochs': config.ADDITIONAL_EPOCHS,
            'learning_rate': config.LEARNING_RATE,
            'batch_size': config.BATCH_SIZE,
            'unfreeze_from_layer': config.UNFREEZE_FROM_LAYER,
        },
        'results': {
            'before': {
                'accuracy': float(results_before[1]),
                'loss': float(results_before[0]),
            },
            'after': {
                'accuracy': float(results_after[1]),
                'loss': float(results_after[0]),
            },
            'improvement': {
                'accuracy': float(results_after[1] - results_before[1]),
                'accuracy_percent': float((results_after[1] - results_before[1]) * 100),
            }
        },
        'log_dir': log_dir
    }

    summary_path = 'training_summary_resumed.json'
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"\nðŸ’¾ Trainings-Zusammenfassung gespeichert: {summary_path}")


def main():
    """Hauptfunktion fÃ¼r Resume Training"""
    print("=" * 60)
    print("ðŸ”„ Garbage Classification Model - Weitertrainieren")
    print("=" * 60)

    config = Config()

    try:
        # Setup
        setup_gpu()
        validate_directories(config)

        # Lade Modell
        print("\n" + "=" * 60)
        print("ðŸ“¦ SCHRITT 1: Modell laden")
        print("=" * 60)
        model = load_model(config.MODEL_PATH)

        # Daten vorbereiten
        print("\n" + "=" * 60)
        print("ðŸ“Š SCHRITT 2: Daten vorbereiten")
        print("=" * 60)
        train_gen, val_gen = create_data_generators(config)
        class_weights = calculate_class_weights(train_gen, config)

        # Optional: Layers entfrieren
        print("\n" + "=" * 60)
        print("ðŸ”“ SCHRITT 3: Fine-Tuning konfigurieren")
        print("=" * 60)
        unfreeze_layers(model, config)

        # Modell neu kompilieren
        print("\n" + "=" * 60)
        print("âš™ï¸ SCHRITT 4: Modell kompilieren")
        print("=" * 60)
        model.compile(
            optimizer=Adam(learning_rate=config.LEARNING_RATE),
            loss='categorical_crossentropy',
            metrics=[
                'accuracy',
                tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy')
            ]
        )
        print(f"âœ… Learning Rate: {config.LEARNING_RATE}")

        # Callbacks einrichten
        callbacks, log_dir = setup_callbacks(config)

        # Evaluiere vor Training
        print("\n" + "=" * 60)
        print("ðŸ“Š SCHRITT 5: Evaluierung vor Training")
        print("=" * 60)
        results_before = model.evaluate(val_gen, verbose=0)
        print(f"   Val-Accuracy: {results_before[1] * 100:.2f}%")
        print(f"   Val-Loss: {results_before[0]:.4f}")

        # Training
        print("\n" + "=" * 60)
        print(f"ðŸ‹ï¸ SCHRITT 6: Weitertraining ({config.ADDITIONAL_EPOCHS} Epochen)")
        print("=" * 60)

        history = model.fit(
            train_gen,
            epochs=config.ADDITIONAL_EPOCHS,
            validation_data=val_gen,
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=1
        )

        # Evaluiere nach Training
        print("\n" + "=" * 60)
        print("ðŸ“Š SCHRITT 7: Finale Evaluierung")
        print("=" * 60)

        best_model = tf.keras.models.load_model(config.RESUMED_BEST)
        results_after = best_model.evaluate(val_gen, verbose=1)

        # Ergebnisse anzeigen
        print("\n" + "=" * 60)
        print("âœ… FINALE METRIKEN")
        print("=" * 60)
        print(f"   Vorher:      {results_before[1] * 100:.2f}%")
        print(f"   Nachher:     {results_after[1] * 100:.2f}%")
        improvement = (results_after[1] - results_before[1]) * 100
        print(f"   Verbesserung: {improvement:+.2f}%")

        if improvement > 0:
            print("\nðŸŽ‰ Modell hat sich verbessert!")
        elif improvement < -1:
            print("\nâš ï¸ Modell hat sich verschlechtert - Ã¼berprÃ¼fe Hyperparameter")
        else:
            print("\nðŸ“Š Keine signifikante Ã„nderung")

        # Speichere finales Modell
        best_model.save(config.FINAL_RESUMED)
        print("\nðŸ’¾ Modelle gespeichert:")
        print(f"   âœ… {config.RESUMED_BEST} (bestes wÃ¤hrend Training)")
        print(f"   âœ… {config.FINAL_RESUMED} (finales Modell)")

        # Visualisierung und Zusammenfassung
        plot_training_history(history)
        save_training_summary(results_before, results_after, config, log_dir)

        print("\n" + "=" * 60)
        print("ðŸŽ‰ Weitertraining erfolgreich abgeschlossen!")
        print("=" * 60)
        print(f"\nðŸ“Š TensorBoard starten mit:")
        print(f"   tensorboard --logdir={log_dir}")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ FEHLER: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()